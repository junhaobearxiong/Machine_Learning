\documentclass[11pt]{article}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{marvosym}
\usepackage{enumerate}
\usepackage{subfigure}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref} 


\oddsidemargin 0mm
\evensidemargin 5mm
\topmargin -20mm
\textheight 240mm
\textwidth 160mm



\newcommand{\vwi}{{\bf w}_i}
\newcommand{\vw}{{\bf w}}
\newcommand{\vx}{{\bf x}}
\newcommand{\vy}{{\bf y}}
\newcommand{\vxi}{{\bf x}_i}
\newcommand{\yi}{y_i}
\newcommand{\vxj}{{\bf x}_j}
\newcommand{\vxn}{{\bf x}_n}
\newcommand{\yj}{y_j}
\newcommand{\ai}{\alpha_i}
\newcommand{\aj}{\alpha_j}
\newcommand{\X}{{\bf X}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\vz}{{\bf z}}
\newcommand{\msigma}{{\bf \Sigma}}
\newcommand{\vmu}{{\bf \mu}}
\newcommand{\vmuk}{{\bf \mu}_k}
\newcommand{\msigmak}{{\bf \Sigma}_k}
\newcommand{\vmuj}{{\bf \mu}_j}
\newcommand{\msigmaj}{{\bf \Sigma}_j}
\newcommand{\pij}{\pi_j}
\newcommand{\pik}{\pi_k}
\newcommand{\D}{\mathcal{D}}
\newcommand{\el}{\mathcal{L}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\vxij}{{\bf x}_{ij}}
\newcommand{\vt}{{\bf t}}
\newcommand{\yh}{\hat{y}}
\newcommand{\code}[1]{{\footnotesize \tt #1}}
\newcommand{\alphai}{\alpha_i}

\pagestyle{myheadings} 
\markboth{Homework 1}{Spring 2018 CS 475 Machine Learning: Project 1}


\title{CS 475 Machine Learning: Project 1\\Supervised Classifiers 1\\
	\Large{Due: Thursday February 22, 2018, 11:59pm}\\
	50 Points Total \hspace{1cm} Version 1.0}
\author{Junhao Xiong}
\date{}

\begin{document}
	\large
	\maketitle
	\thispagestyle{headings}
	
	\vspace{-.5in}
	
	
		\section{Analytical (20 Points)}
	
	\paragraph{1) Supervised vs. Unsupervised Learning (3 points)}.
	\begin{enumerate}
		\item Give an example of a problem that could be solved with both supervised learning and unsupervised learning. Is data readily available for this problem? How would you measure your `success' in solving this problem for each approach?\newline
		
		Answer: Recognizing hand written digits. One can either do classification i.e. supervised learning, or clustering i.e. unsupervised learning. The data is available - MNIST. Success in classification can be measured by the proportion of digits classified correctly. Success in clustering can be measured by the proportion of digits in the correct cluster.  
		
		\item What are the pros and cons of each approach? Which approach do you think the problem better lends itself to?\newline
		
		Answer:\newline
		Supervised learning\newline
		Pro: It is relatively easier to get good accuracy. There exist many algorithms with guarantee for good performance. It is also generally more efficient.\newline
		Con: Someone needs to hand label the training set.\newline                                                                                                           
		
		Unsupervised learning\newline                                                        
		Pro: No need to hand label data\newline
		Con: It is relatively harder to get good performance. It is generally slower. Many good clustering algorithms involved pairwise calculations of the entire training sets, which can be quite slow.\newline
		
		Supervised learning is more natural for this problem, since there is already abundant datasets available for training.
		
	\end{enumerate}
	
	\paragraph{2) Model Complexity (3 points)} Explain when you would want to use a simple model over a complex model and vice versa. Are there any approaches you could use to mitigate the disadvantages of using a complex model?\newline
	
	Answer: One would want a simple model when the number of data available for training is relatively small and when one wants to prevent overfitting.
A complex model is suitable when large number of data is available for training to achieve good performance and when one wishes to minimize bias.\newline
	
	To mitigate the problem of overfitting, one could perform regularization.

	
	
	\paragraph{3) Training and Generalization (3 points)} Suppose you're building a system to classify images of food into two categories: either the image contains a hot dog or it does not. You're given a dataset of 25,000 (image, label) pairs for training and a separate dataset of 5,000 (image, label) pairs.
	\begin{enumerate}
		\item Suppose you train an algorithm and obtain 96\% accuracy on the larger training set. Do you expect the trained model to obtain similar performance if used on newly acquired data? Why or why not?\newline
		
		Answer: One wouldn?t necessarily know the model?s performance on the test set, since despite having good performance on the training set, the model might be overfitting, or it might generalize well. One would need to know more about the model to answer the question.
		
		\item Suppose that, after training, someone gives you a new test set of 1,000 (image, label) pairs. Which do you expect to give greater accuracy on the test set: The model after trained on the dataset of 25,000 pairs or the model after trained on the dataset of 5,000 pairs? Explain your reasoning.\newline
		
		Answer: It depends. More data doesn?t necessarily lead to better performance. The data might be poorly labeled, contain lots of noises, or the model might be poor. But given good data and a sound model, one may expect better performance for the model trained on more data.
		
		\item Suppose your models obtained greater than 90\% accuracy on the test set. How might you proceed in hope of improving accuracy further?\newline
		
		Answer: One may tune the hyper-parameters using a validation set. But one should note to reinitialize all parameters before validation since the model has already seen the test set at this point. 
	\end{enumerate}
	
	\paragraph{4) Loss Function (3 points)} State whether each of the following is a valid loss function 
	 for binary classification.  Wherever a loss function is not valid, state why. Here $y$ is the correct label and $\hat{y}$ is a decision confidence value, meaning that the predicted label is given by sign($\hat{y}$) and the confidence on the classification increases with $\vert\hat{y}\vert$.
	
	\begin{enumerate}
		\item $\ell (y, \hat{y}) = \frac{3}{4} (y-\hat{y})^2$. Yes
		\item $\ell (y, \hat{y}) = |(y-\hat{y})|/\hat{y}$. No, because it would be negative when $\hat{y}$ is negative.
		\item $\ell (y, \hat{y}) = \max(0, 1 - y \cdot \hat{y})$. Yes
	\end{enumerate}
	
	\paragraph{5) Linear Regression (4 points)} Suppose you observe $n$ data points $(x_1, y_1), \ldots, (x_n, y_n)$, where all $x_i$ and all $y_i$ are scalars.
	\begin{enumerate}
		\item Suppose you choose the model $\hat{y} = w x$ and aim to minimize the sum of squares error $\sum_i (y_i - \hat{y}_i)^2$. Derive the closed-form solution for $w$ from scratch, where `from scratch' means without using the least-squares solution presented in class.
		\begin{equation}
		\text{min} \sum_i (y_i - \hat{y}_i) ^2
		= \text{min} \sum_i 2 y_i^2 - 2 w x_i y_i + w^2 x_i^2
		\end{equation}
		
		\begin{equation}
		\partial{((y_i - \hat{y}_i) ^2)}/\partial(w) = \sum
		\end{equation}
		
		\item Suppose you instead choose the model $\hat{y} = w \sin(x)$ and aim to minimize the sum of squares error $\sum_i (y_i - \hat{y}_i)^2$. Is there a closed-form solution for $w$? If so, what is it?
	\end{enumerate}
	
	\paragraph{6) Logistic Regression (4 points)} Explain whether each statement is true or false. If false, explain why.
	\begin{enumerate}
		\item In the case of binary classification, optimizing the logistic loss is equivalent to minimizing the sum-of-squares error between our predicted probabilities for class 1, $\hat{\vy}$, and the observed probabilities for class 1, $\vy$.
		\item One possible advantage of stochastic gradient descent is that it can sometimes escape local minima. However, in the case of logistic regression, the global minimum is the only minimum, and stochastic gradient descent is therefore never useful.
	\end{enumerate}
	
\end{document}
